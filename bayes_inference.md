# Introduction: Bayesian Inference

At the heart of Bayesian statistics lies **Bayes' theorem**, which describes how we update our beliefs about parameters Î¸ given observed data D:

$$
P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}
$$

Where:
- **P(Î¸ | D)** is the *posterior distribution* â€” what we want to learn
- **P(D | Î¸)** is the *likelihood* â€” how probable the data is given the parameters
- **P(Î¸)** is the *prior distribution* â€” our initial beliefs about the parameters
- **P(D)** is the *marginal likelihood* or evidence â€” a normalizing constant

### The Challenge

The denominator P(D) requires integrating over all possible parameter values:

$$
P(D) = \int P(D \mid \theta) \cdot P(\theta) \, d\theta
$$

For most real-world problems, this integral is **intractable** â€” impossible to compute analytically. This is where **sampling algorithms** come in.

### Why We Need Samplers

Instead of computing the posterior distribution directly, samplers generate representative samples from P(Î¸ | D). With enough samples, we can:
- Estimate posterior means, medians, and credible intervals
- Visualize the posterior distribution
- Make predictions on new data
- Perform model comparison

---

## ğŸ“Œ Available Samplers

This page gathers different sampling algorithms I experiment with, mostly in the context of Bayesian inference and high-dimensional problems. Each sampler has its own dedicated page with interactive visualizations, implementation details, and diagnostics.

---

### ğŸ”¹ Markov Chain Monte Carlo (MCMC)

A foundational Metropolisâ€“Hastings sampler that forms the basis for understanding modern sampling methods. This implementation serves as a reference for exploring fundamental concepts like:

- **Convergence diagnostics** â€” How to know when your chain has reached the posterior
- **Autocorrelation** â€” Understanding sample dependencies and effective sample size
- **Acceptance rates** â€” Tuning proposals for efficient exploration
- **Scaling behavior** â€” Why vanilla MCMC struggles in high dimensions

While not the most efficient for complex posteriors, MCMC remains invaluable for building intuition and as a diagnostic baseline.

â¡ï¸ **[Explore the MCMC sampler](/html_src/interactive_mcmc.html)**  
*Tags:* Bayesian inference, Metropolis-Hastings, diagnostics, autocorrelation

---

### ğŸ”¹ Hamiltonian Monte Carlo (HMC)

**Coming soon** â€” A gradient-based sampler that treats sampling as a physics simulation problem.

HMC leverages Hamiltonian dynamics to propose distant states with high acceptance probability, making it particularly effective for:

- **High-dimensional posteriors** â€” Efficient exploration where MCMC fails
- **Complex geometries** â€” Following curved posterior landscapes
- **Reduced autocorrelation** â€” Longer jumps mean fewer correlated samples

By simulating the motion of a particle with momentum through the posterior landscape, HMC can traverse the distribution much more efficiently than random-walk methods.

â¡ï¸ **[Explore the HMC sampler](/html_src/interactive_hmc.html)**  
*Tags:* HMC, gradients, high-dimensional inference, Hamiltonian dynamics

---

## ğŸ”§ Implementation Notes

All samplers are implemented with:
- **Interactive visualizations** â€” See the algorithms in action
- **Step-by-step explanations** â€” Understand what's happening at each iteration
- **Diagnostic tools** â€” Assess convergence and sample quality
- **Comparative benchmarks** â€” Performance across different problem types

---

## ğŸ“š Further Reading

- **Bayesian Data Analysis** (Gelman et al.) â€” Comprehensive treatment of Bayesian methods
- **MCMC Handbook** (Brooks et al.) â€” Deep dive into sampling algorithms
- **[Stan Documentation](https://mc-stan.org/docs/)** â€” Practical implementation patterns