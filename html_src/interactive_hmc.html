<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Interactive HMC Demo - Hamiltonian Monte Carlo | Jules Perret</title>
  <meta name="description" content="Interactive visualization of the Hamiltonian Monte Carlo (HMC) algorithm" />

  <!-- MathJax for LaTeX rendering -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>

  <link rel="stylesheet" href="css/shared-styles.css">
</head>

<body>

<header class="site-header">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Jules Perret</a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>
      <div class="trigger">
        <a class="page-link" href="/about.html">Curriculum Vitae</a>
        <a class="page-link" href="/bayesian_inference.html">Introduction to Bayesian Inference</a>
        <a class="page-link" href="/publications.html">Publications</a>
        <a class="page-link" href="/research.html">Research</a>
      </div>
    </nav>
  </div>
</header>


<main class="page-content" aria-label="Content">
  <div class="wrapper">
    <h1 class="page-heading">Hamiltonian Monte Carlo (HMC)</h1>
    <p class="page-subtitle">
      Interactive visualization of gradient-based MCMC sampling from 2D posterior distributions<br>
      <span style="font-size: 0.9rem; color: #718096;">Part 2 of the MCMC Samplers Series | See also: <a href="/mcmc.html" style="color: #667eea;">Metropolis–Hastings</a></span>
    </p>
  </div>
</main>

<div class="container">

  <!-- INTRODUCTION -->
  <div class="section">
    <h2 class="section-title">Introduction to Hamiltonian Monte Carlo</h2>
    <p>
      <strong>Hamiltonian Monte Carlo (HMC)</strong> is an MCMC algorithm that uses gradient information
      to generate efficient proposals via Hamiltonian dynamics. Unlike random-walk methods
      (like Metropolis–Hastings), HMC makes distant proposals while maintaining high acceptance rates by
      exploiting the geometry of the target distribution.
    </p>
    <p>
      HMC addresses key limitations of random-walk MCMC: the diffusive nature of exploration and
      poor performance with correlated parameters. By simulating Hamiltonian dynamics, HMC generates
      coherent trajectories through parameter space, dramatically improving efficiency, especially in high dimensions.
    </p>
    
    <div class="algorithm-box" style="margin-top: 1.5rem;">
      <strong>Why HMC?</strong>
      <ul style="margin: 0.5rem 0; padding-left: 1.5rem;">
        <li><strong>Efficient exploration:</strong> Generates distant proposals with high acceptance probability using gradient information</li>
        <li><strong>Reduced autocorrelation:</strong> Produces nearly independent samples by making coherent moves through parameter space</li>
        <li><strong>Handles correlations:</strong> Exploits geometric information from gradients, efficient even with strongly correlated parameters</li>
        <li><strong>Scales to high dimensions:</strong> Convergence rate depends on condition number rather than dimensionality</li>
        <li><strong>Industry standard:</strong> Powers modern probabilistic programming frameworks like Stan, PyMC, and NumPyro</li>
      </ul>
    </div>

    <div class="info-note" style="margin-top: 1.5rem;">
      <strong>The Key Insight:</strong> HMC augments the target distribution with auxiliary momentum variables
      and simulates Hamiltonian dynamics using a symplectic integrator. Energy conservation ensures the
      sampler spends more time in high-probability regions, while the momentum enables efficient exploration
      by carrying the trajectory across low-probability regions.
    </div>
  </div>

  <!-- ALGORITHM DESCRIPTION -->
  <div class="section">
    <h2 class="section-title">The Hamiltonian Monte Carlo Algorithm</h2>
    <p>
      <strong>HMC</strong> was introduced by Duane et al. (1987) for lattice field theory and popularized 
      for statistics by Neal (1996, 2011). The algorithm augments the parameter vector \(\mathbf{q}\) 
      (position) with a momentum vector \(\mathbf{p}\), defining a Hamiltonian system.
    </p>

    <div class="algorithm-box">
      <strong>The Hamiltonian:</strong>
      <p style="margin: 0.75rem 0;">
        The total energy of the system combines potential energy (negative log posterior) and kinetic energy:
      </p>
      $$H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + K(\mathbf{p})$$
      <p style="margin: 0.5rem 0;">
        where:
      </p>
      <ul style="margin: 0; padding-left: 1.5rem;">
        <li>\(U(\mathbf{q}) = -\log \pi(\mathbf{q})\) is the potential energy (negative log target density)</li>
        <li>\(K(\mathbf{p}) = \frac{1}{2}\mathbf{p}^T M^{-1} \mathbf{p}\) is the kinetic energy (mass matrix \(M\))</li>
      </ul>
      <p style="margin: 0.75rem 0 0 0;">
        In this demo, we use \(M = I\) (identity matrix), so \(K(\mathbf{p}) = \frac{1}{2}\sum_i p_i^2\).
      </p>
    </div>
    
    <div class="algorithm-box" style="margin-top: 1.5rem;">
      <strong>HMC Algorithm Steps:</strong>
      <ol style="margin: 0.5rem 0 0.5rem 0; padding-left: 1.5rem;">
        <li style="margin: 0.75rem 0;">
          <strong>Sample momentum:</strong> Draw \(\mathbf{p} \sim \mathcal{N}(0, M)\)
        </li>
        <li style="margin: 0.75rem 0;">
          <strong>Simulate Hamiltonian dynamics:</strong> Starting from \((\mathbf{q}, \mathbf{p})\), integrate 
          Hamilton's equations for \(L\) steps of size \(\epsilon\):
          $$\begin{aligned}
          \frac{d\mathbf{q}}{dt} &= \frac{\partial H}{\partial \mathbf{p}} = M^{-1}\mathbf{p}\\
          \frac{d\mathbf{p}}{dt} &= -\frac{\partial H}{\partial \mathbf{q}} = -\nabla U(\mathbf{q})
          \end{aligned}$$
          We use the <strong>leapfrog integrator</strong> (symplectic, reversible):
          <div style="margin: 0.75rem 0 0 1.5rem; font-family: 'Courier New', monospace; font-size: 0.9rem;">
            \(\mathbf{p} \leftarrow \mathbf{p} - \frac{\epsilon}{2}\nabla U(\mathbf{q})\)<br>
            for \(i = 1\) to \(L\):<br>
            &nbsp;&nbsp;\(\mathbf{q} \leftarrow \mathbf{q} + \epsilon M^{-1}\mathbf{p}\)<br>
            &nbsp;&nbsp;\(\mathbf{p} \leftarrow \mathbf{p} - \epsilon \nabla U(\mathbf{q})\) &nbsp;(except last step: \(\epsilon/2\))
          </div>
        </li>
        <li style="margin: 0.75rem 0;">
          <strong>Metropolis acceptance:</strong> Propose \((\mathbf{q}^*, \mathbf{p}^*)\) from dynamics. Accept with probability:
          $$\alpha = \min\left(1, \exp(H(\mathbf{q}, \mathbf{p}) - H(\mathbf{q}^*, \mathbf{p}^*))\right)$$
          Due to numerical errors, \(H\) is not perfectly conserved, but acceptance corrects for this.
        </li>
        <li style="margin: 0.75rem 0;">
          <strong>Return position:</strong> If accepted, set \(\mathbf{q}_{t+1} = \mathbf{q}^*\); else \(\mathbf{q}_{t+1} = \mathbf{q}_t\)
        </li>
      </ol>
    </div>

    <div class="info-note" style="margin-top: 1.5rem;">
      <strong>Why it works:</strong> The leapfrog integrator is symplectic and time-reversible, approximately
      preserving the Hamiltonian \(H\). This volume-preserving property leads to high acceptance rates.
      Regions of high probability (low \(U(\mathbf{q})\)) correspond to low potential energy where trajectories
      naturally spend more time, while momentum enables coherent exploration across low-probability barriers.
    </div>
  </div>

  <!-- TARGET DISTRIBUTIONS -->
  <div class="section">
    <h2 class="section-title">Target Distributions</h2>
    <p>
      This demonstration provides three target distributions ranging from simple to challenging, 
      illustrating different aspects of MCMC performance:
    </p>
    
    <div style="display: grid; grid-template-columns: 1fr; gap: 1.5rem; margin-top: 1rem;">
      <div class="algorithm-box">
        <strong>Bivariate Gaussian (Default)</strong>
        <p style="margin: 0.5rem 0;">
          A standard correlated 2D Gaussian with correlation coefficient \(\rho = 0.8\):
        </p>
        <div style="overflow-x: auto;">
          $$\pi(x_1, x_2) \propto \exp\left(-\frac{1}{2(1-\rho^2)}(x_1^2 - 2\rho x_1 x_2 + x_2^2)\right)$$
        </div>
        <p style="margin: 0.5rem 0 0 0; font-size: 0.85rem; color: #4a5568;">
          This distribution has elliptical contours aligned with the correlation structure. 
          With strong correlation (\(\rho = 0.8\)), random-walk proposals struggle because they explore 
          axis-by-axis when parameters should move together along the correlation direction.
        </p>
      </div>

      <div class="algorithm-box">
        <strong>Rosenbrock's Banana Distribution</strong>
        <p style="margin: 0.5rem 0;">
          A transformed Gaussian that creates a curved, banana-shaped density (Haario et al., 1999):
        </p>
        <div style="overflow-x: auto;">
          $$\pi(x_1, x_2) \propto \exp\left(-\frac{1}{200}(x_1^2 + 100(x_2 - x_1^2)^2)\right)$$
        </div>
        <p style="margin: 0.5rem 0 0 0; font-size: 0.85rem; color: #4a5568;">
          This distribution is strongly correlated along a curved manifold, making it difficult for 
          random-walk samplers to explore efficiently. The "banana" shape arises from the nonlinear 
          transformation \(x_2 - x_1^2\), creating regions where standard proposals are inefficient.
        </p>
      </div>
      
      <div class="algorithm-box">
        <strong>Neal's Funnel Distribution</strong>
        <p style="margin: 0.5rem 0;">
          A hierarchical model that exhibits strong scale variations (Neal, 2003):
        </p>
        <div style="overflow-x: auto;">
          $$\begin{aligned}
          x_1 &\sim \mathcal{N}(0, 3^2) \\
          x_2 \mid x_1 &\sim \mathcal{N}(0, \exp(x_1)^2)
          \end{aligned}$$
        </div>
        <p style="margin: 0.5rem 0 0 0; font-size: 0.85rem; color: #4a5568;">
          The joint density is \(\pi(x_1, x_2) \propto \exp(-x_1^2/18 - x_2^2/(2e^{2x_1}))\). 
          This creates a "funnel" where the scale of \(x_2\) depends exponentially on \(x_1\), 
          challenging for fixed-width proposals. Common in hierarchical Bayesian models with variance parameters.
        </p>
      </div>
    </div>
    
    <div class="info-note" style="margin-top: 1rem;">
      <strong>Why these distributions?</strong> The Gaussian provides a baseline to understand basic behavior. 
      Rosenbrock's banana and Neal's funnel are standard benchmarks in the MCMC literature—the banana tests 
      handling of strong nonlinear correlations, while the funnel tests adaptation to varying scales. 
      These challenges motivate advanced methods like HMC and adaptive MCMC.
    </div>
  </div>

  <!-- CONTROLS -->
  <div class="section">
    <h2 class="section-title">Simulation Controls</h2>
    <p>
      HMC has two key tuning parameters: the <strong>step size</strong> \(\epsilon\) (how large each 
      leapfrog step is) and the <strong>number of steps</strong> \(L\) (how long the trajectory is). 
      The product \(L \times \epsilon\) determines the trajectory length.
    </p>
    
    <div class="controls-grid">
      <div class="control-group">
        <label>
          Step Size (\(\epsilon\))
          <span class="control-value" id="epsilonVal">0.10</span>
        </label>
        <input type="range" min="0.01" max="0.5" step="0.01" value="0.10" id="epsilon">
        <div style="font-size: 0.85rem; color: #4a5568; margin-top: 0.25rem;">
          Size of each leapfrog integration step. Smaller → more accurate, but slower.
        </div>
      </div>

      <div class="control-group">
        <label>
          Number of Steps (L)
          <span class="control-value" id="numStepsVal">20</span>
        </label>
        <input type="range" min="5" max="100" step="1" value="20" id="numSteps">
        <div style="font-size: 0.85rem; color: #4a5568; margin-top: 0.25rem;">
          Length of trajectory. More steps → longer proposals, but can overshoot.
        </div>
      </div>

      <div class="control-group">
        <label>
          Iteration Speed (ms)
          <span class="control-value" id="speedVal">60</span>
        </label>
        <input type="range" min="10" max="500" step="10" value="60" id="speed">
        <div style="font-size: 0.85rem; color: #4a5568; margin-top: 0.25rem;">
          Delay between iterations. Slower speeds help visualize trajectories.
        </div>
      </div>

      <div class="control-group">
        <label>
          Target Distribution
        </label>
        <select id="dist">
          <option value="gaussian">Bivariate Gaussian</option>
          <option value="banana">Rosenbrock's Banana</option>
          <option value="funnel">Neal's Funnel</option>
        </select>
        <div style="font-size: 0.85rem; color: #4a5568; margin-top: 0.25rem;">
          Same distributions as Metropolis–Hastings for direct comparison.
        </div>
      </div>
    </div>

    <div class="button-group">
      <button onclick="start()">▶ Start Sampling</button>
      <button class="secondary" onclick="singleStep()">→ Single Step</button>
      <button class="tertiary" onclick="reset()">⟲ Reset</button>
    </div>

    <div class="info-note" style="margin-top: 1.5rem;">
      <strong>Tuning advice:</strong> Step size controls the trade-off between integration accuracy and
      discretization error. Too large leads to high rejection rates; too small wastes computational resources.
      Target acceptance rates of 60-90% (compared to ~23% for optimal random-walk MH in high dimensions).
      The No-U-Turn Sampler (NUTS) automatically tunes both step size and trajectory length.
    </div>
  </div>

  <!-- VISUALIZATION -->
  <div class="section">
    <h2 class="section-title">HMC Trajectory Visualization</h2>
    <p>
      The main plot shows the joint target distribution \(\pi(q_1, q_2)\) with darker regions indicating higher probability.
      HMC trajectories follow curved paths determined by Hamilton's equations, guided by the gradient of the log density.
      <span style="color: #38a169; font-weight: 600;">Green points</span> indicate accepted proposals and
      <span style="color: #ed8936; font-weight: 600;">orange points</span> indicate rejected proposals.
      The plot axes automatically adjust to each distribution's natural range.
    </p>
    
    <div class="visualization-grid">
      <!-- Main posterior plot -->
      <div class="canvas-container">
        <div class="canvas-label">Joint Posterior Distribution π(x₁, x₂)</div>
        <canvas id="posterior" width="650" height="650"></canvas>
        <p style="font-size: 0.85rem; color: #4a5568; margin-top: 0.5rem;">
          The chain should explore the high-density regions (darker areas) while maintaining enough 
          randomness to avoid getting trapped in any single location.
        </p>
      </div>

      <!-- Marginal plots -->
      <div class="marginal-plots">
        <div class="marginal-plot">
          <div class="canvas-label">Marginal Distribution of x₁</div>
          <canvas id="histX" width="300" height="280"></canvas>
          <p style="font-size: 0.85rem; color: #4a5568; margin-top: 0.5rem;">
            Histogram of \(x_1\) samples. Should converge to the true marginal \(\pi(x_1) = \int \pi(x_1, x_2) dx_2\).
          </p>
        </div>
        <div class="marginal-plot">
          <div class="canvas-label">Marginal Distribution of x₂</div>
          <canvas id="histY" width="300" height="280"></canvas>
          <p style="font-size: 0.85rem; color: #4a5568; margin-top: 0.5rem;">
            Histogram of \(x_2\) samples. With enough samples, this approximates the true marginal distribution.
          </p>
        </div>
      </div>
    </div>

    <!-- Current step info -->
    <div class="stats-panel">
      <strong>Current Iteration Details</strong>
      <div id="stepInfo" class="step-info">
        Click "Start Sampling" or "Single Step" to begin the MCMC algorithm.
      </div>
    </div>
  </div>

  <!-- TRACE PLOTS -->
  <div class="section">
    <h2 class="section-title">Chain Trace Plots</h2>
    <p>
      <strong>Trace plots</strong> show the evolution of each parameter value over iterations. They are essential 
      for diagnosing convergence and mixing of the MCMC chain.
    </p>
    
    <div class="algorithm-box" style="margin-bottom: 1.5rem;">
      <strong>What to look for in trace plots:</strong>
      <ul style="margin: 0.5rem 0; padding-left: 1.5rem;">
        <li><strong>Good mixing:</strong> The trace should look like "fuzzy caterpillar" - random fluctuations 
        around a stable mean with no obvious patterns or trends</li>
        <li><strong>Stationarity:</strong> The mean and variance should remain constant over time (after burn-in)</li>
        <li><strong>No trends:</strong> The trace shouldn't show long-term upward or downward trends</li>
        <li><strong>No getting stuck:</strong> The chain shouldn't remain at the same value for extended periods</li>
      </ul>
    </div>
    
    <div style="display: grid; grid-template-columns: 1fr; gap: 1.5rem;">
      <div>
        <div class="canvas-label">Trace Plot: x₁ Evolution</div>
        <canvas id="traceX" width="100%" height="200" style="width: 100%;"></canvas>
        <p style="font-size: 0.85rem; color: #4a5568; margin-top: 0.5rem;">
          Chain trace for parameter \(x_1\). Rapid fluctuations indicate good mixing; long periods at similar
          values suggest poor exploration.
        </p>
      </div>
      <div>
        <div class="canvas-label">Trace Plot: x₂ Evolution</div>
        <canvas id="traceY" width="100%" height="200" style="width: 100%;"></canvas>
        <p style="font-size: 0.85rem; color: #4a5568; margin-top: 0.5rem;">
          Chain trace for parameter \(x_2\). Compare with \(x_1\) trace to assess if both parameters mix at similar rates.
        </p>
      </div>
    </div>
  </div>

  <!-- DIAGNOSTICS -->
  <div class="section">
    <h2 class="section-title">Chain Diagnostics</h2>
    
    <h3 style="font-size: 1.1rem; font-weight: 600; margin: 0 0 1rem 0;">Autocorrelation Function (ACF)</h3>
    <p>
      The <strong>autocorrelation function</strong> measures the correlation between samples separated by 
      \(\tau\) iterations (the lag). For a well-mixing chain, the ACF should decay rapidly to zero.
    </p>
    
    <div class="algorithm-box" style="margin-bottom: 1.5rem;">
      <strong>Mathematical Definition:</strong>
      <p style="margin: 0.75rem 0;">
        For a chain \(\{x_t\}\) with sample mean \(\bar{x}\) and sample variance \(s^2\), the sample ACF at lag \(\tau\) is:
      </p>
      $$\hat{\rho}(\tau) = \frac{\sum_{t=1}^{n-\tau} (x_t - \bar{x})(x_{t+\tau} - \bar{x})}{\sum_{t=1}^{n} (x_t - \bar{x})^2}$$
      <p style="margin: 0.75rem 0 0.25rem 0;">
        <strong>Interpretation:</strong>
      </p>
      <ul style="margin: 0; padding-left: 1.5rem;">
        <li>\(\hat{\rho}(0) = 1\): Perfect correlation with itself</li>
        <li>\(\hat{\rho}(\tau) \to 0\) as \(\tau \to \infty\): Samples become independent (for ergodic chains)</li>
        <li><strong>Slow decay:</strong> High autocorrelation indicates the chain mixes slowly</li>
        <li><strong>Fast decay:</strong> Low autocorrelation indicates efficient exploration</li>
      </ul>
    </div>
    
    <div class="canvas-label">Autocorrelation Function (ACF)</div>
    <canvas id="acfXY" width="100%" height="350" style="width: 100%;"></canvas>
    
    <p style="margin-top: 1rem; color: #4a5568; font-size: 0.9rem;">
      <strong>Blue line:</strong> ACF for \(x_1\) parameter. 
      <strong>Red line:</strong> ACF for \(x_2\) parameter.
      The lag range adapts automatically—showing where the ACF crosses zero plus 33% additional lags 
      to verify convergence to independence.
    </p>

    <h3 style="font-size: 1.1rem; font-weight: 600; margin: 2rem 0 1rem 0;">Performance Metrics</h3>
    <div class="stats-grid" style="margin-top: 1rem;">
      <div class="stat-item">
        <div class="stat-label">Total Iterations</div>
        <div class="stat-value" id="totalIter">0</div>
      </div>
      <div class="stat-item">
        <div class="stat-label">Acceptance Rate</div>
        <div class="stat-value" id="acceptRate">—</div>
      </div>
      <div class="stat-item">
        <div class="stat-label">Accepted Proposals</div>
        <div class="stat-value" id="acceptCount">0</div>
      </div>
      <div class="stat-item">
        <div class="stat-label">Effective Sample Size (est.)</div>
        <div class="stat-value" id="essValue">—</div>
      </div>
    </div>
    
    <div class="algorithm-box" style="margin-top: 1.5rem;">
      <strong>Effective Sample Size (ESS)</strong>
      <p style="margin: 0.75rem 0;">
        Due to autocorrelation, MCMC samples are not independent. The ESS estimates how many independent 
        samples our correlated chain is equivalent to:
      </p>
      $$\text{ESS} \approx \frac{n}{1 + 2\sum_{\tau=1}^{\infty} \rho(\tau)}$$
      <p style="margin: 0.75rem 0 0 0; font-size: 0.9rem;">
        where \(n\) is the total number of samples and \(\rho(\tau)\) is the ACF. Higher ESS means more 
        efficient sampling. The ratio ESS/\(n\) is called the <strong>sampling efficiency</strong>.
      </p>
    </div>
    
    <div class="info-note" style="margin-top: 1.5rem;">
      <strong>Note on acceptance rates:</strong> Unlike random-walk Metropolis (optimal acceptance ~23% in high dimensions),
      HMC maintains much higher acceptance rates (60-90%) due to the volume-preserving properties of symplectic integration.
      This is possible because the leapfrog integrator approximately conserves the Hamiltonian, making proposals
      follow surfaces of equal probability.
    </div>
  </div>

  <!-- STRENGTHS AND LIMITATIONS -->
  <div class="section">
    <h2 class="section-title">Strengths & Limitations of HMC</h2>
    
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin-top: 1rem;">
      <div class="algorithm-box">
        <strong style="color: #059669;">✓ Strengths</strong>
        <ul style="margin: 0.75rem 0 0 0; padding-left: 1.5rem;">
          <li style="margin: 0.5rem 0;"><strong>Efficient exploration:</strong> Generates coherent trajectories rather than diffusive random walks</li>
          <li style="margin: 0.5rem 0;"><strong>Low autocorrelation:</strong> Produces nearly independent samples, high effective sample size</li>
          <li style="margin: 0.5rem 0;"><strong>Exploits geometry:</strong> Uses gradient information to navigate complex correlation structures</li>
          <li style="margin: 0.5rem 0;"><strong>High acceptance probability:</strong> Typically 60-90% due to symplectic integration</li>
          <li style="margin: 0.5rem 0;"><strong>Dimension-robust:</strong> Scales with condition number rather than dimensionality</li>
        </ul>
      </div>
      
      <div class="algorithm-box">
        <strong style="color: #dc2626;">✗ Limitations</strong>
        <ul style="margin: 0.75rem 0 0 0; padding-left: 1.5rem;">
          <li style="margin: 0.5rem 0;"><strong>Requires differentiability:</strong> Needs \(\nabla \log \pi(\mathbf{q})\) to compute Hamiltonian dynamics</li>
          <li style="margin: 0.5rem 0;"><strong>Tuning required:</strong> Step size \(\epsilon\) and trajectory length \(L\) affect performance</li>
          <li style="margin: 0.5rem 0;"><strong>Computational cost:</strong> Each trajectory requires \(L\) gradient evaluations</li>
          <li style="margin: 0.5rem 0;"><strong>Discretization error:</strong> Finite step size introduces numerical integration error</li>
          <li style="margin: 0.5rem 0;"><strong>Mode transitions:</strong> Difficult to cross low-probability regions between well-separated modes</li>
        </ul>
      </div>
    </div>
    
    <div class="info-note" style="margin-top: 1.5rem;">
      <strong>When to use HMC:</strong>
      <ul style="margin: 0.5rem 0 0 0; padding-left: 1.5rem;">
        <li><strong>Smooth target distributions:</strong> Requires differentiable log-density \(\log \pi(\mathbf{q})\)</li>
        <li><strong>Moderate to high dimensions:</strong> Particularly effective for \(d > 10\) where random-walk methods fail</li>
        <li><strong>Correlated parameters:</strong> Gradient information naturally handles complex posterior geometry</li>
        <li><strong>Practical implementations:</strong> NUTS (No-U-Turn Sampler) in Stan/PyMC automatically adapts
        step size and trajectory length, making HMC accessible without manual tuning</li>
      </ul>
    </div>
  </div>

  <!-- COMPARISON WITH MH -->
  <div class="section">
    <h2 class="section-title">Comparison: HMC vs. Metropolis–Hastings</h2>
    <p>
      Direct comparison on the same target distributions reveals HMC's advantages:
    </p>
    
    <div class="algorithm-box" style="margin-top: 1rem;">
      <strong>Key Differences:</strong>
      <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 1rem; margin-top: 1rem;">
        <div><strong>Property</strong></div>
        <div><strong>Metropolis–Hastings</strong></div>
        <div><strong>HMC</strong></div>
        
        <div style="padding: 0.5rem 0; border-top: 1px solid #e2e8f0;">Proposal mechanism</div>
        <div style="padding: 0.5rem 0; border-top: 1px solid #e2e8f0;">Random walk</div>
        <div style="padding: 0.5rem 0; border-top: 1px solid #e2e8f0;">Hamiltonian dynamics</div>
        
        <div style="padding: 0.5rem 0;">Gradients needed?</div>
        <div style="padding: 0.5rem 0;">No</div>
        <div style="padding: 0.5rem 0;">Yes</div>
        
        <div style="padding: 0.5rem 0;">Typical acceptance rate</div>
        <div style="padding: 0.5rem 0;">23-40% (high-d)</div>
        <div style="padding: 0.5rem 0;">65-90%</div>
        
        <div style="padding: 0.5rem 0;">Autocorrelation</div>
        <div style="padding: 0.5rem 0;">High</div>
        <div style="padding: 0.5rem 0;">Low</div>
        
        <div style="padding: 0.5rem 0;">Effective sample size</div>
        <div style="padding: 0.5rem 0;">Much lower than \(n\)</div>
        <div style="padding: 0.5rem 0;">Closer to \(n\)</div>
        
        <div style="padding: 0.5rem 0;">Best for</div>
        <div style="padding: 0.5rem 0;">Low-d, gradients unavailable</div>
        <div style="padding: 0.5rem 0;">High-d, smooth posteriors</div>
      </div>
    </div>
  </div>
          <strong>Key advantage:</strong> Explores parameter space coherently rather than randomly, 
          dramatically reducing autocorrelation.
        </p>
      </div>
      
      <div class="algorithm-box">
        <strong>Nested Sampling</strong>
        <p style="margin: 0.75rem 0; font-size: 0.9rem;">
          A fundamentally different approach that simultaneously computes samples from the posterior 
          <em>and</em> the model evidence (marginal likelihood). Instead of evolving a Markov chain, 
          nested sampling progressively samples from constrained priors with increasing likelihood thresholds.
        </p>
        <p style="margin: 0.5rem 0 0 0; font-size: 0.85rem; color: #4a5568;">
  </div>

  <!-- REFERENCES -->
  <div class="section">
    <h2 class="section-title">References & Further Reading</h2>
    <div style="font-size: 0.9rem; line-height: 1.8;">
      <p><strong>Key Papers:</strong></p>
      <ul style="padding-left: 1.5rem; margin: 0.5rem 0;">
        <li>Duane, S., Kennedy, A.D., Pendleton, B.J., & Roweth, D. (1987). 
        "Hybrid Monte Carlo." <em>Physics Letters B</em>, 195(2), 216-222.</li>
        <li>Neal, R.M. (1996). "Bayesian Learning for Neural Networks." PhD Thesis, University of Toronto.</li>
        <li>Neal, R.M. (2011). "MCMC Using Hamiltonian Dynamics." <em>Handbook of Markov Chain Monte Carlo</em> 
        (eds. Brooks et al.), CRC Press, 113-162.</li>
        <li>Betancourt, M. (2017). "A Conceptual Introduction to Hamiltonian Monte Carlo." 
        <em>arXiv:1701.02434</em>.</li>
        <li>Hoffman, M.D., & Gelman, A. (2014). "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo." 
        <em>Journal of Machine Learning Research</em>, 15(1), 1593-1623.</li>
        <li>Haario, H., Saksman, E., & Tamminen, J. (1999). "Adaptive Proposal Distribution for Random Walk Metropolis Algorithm." 
        <em>Computational Statistics</em>, 14, 375-395.</li>
      </ul>
      
      <p style="margin-top: 1rem;"><strong>Books:</strong></p>
      <ul style="padding-left: 1.5rem; margin: 0.5rem 0;">
        <li>Brooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). <em>Handbook of Markov Chain Monte Carlo</em>. CRC Press.</li>
        <li>Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., & Rubin, D.B. (2013). 
        <em>Bayesian Data Analysis</em> (3rd ed.). CRC Press.</li>
        <li>Betancourt, M. (2018). <em>A Conceptual Introduction to Hamiltonian Monte Carlo</em>. 
        arXiv preprint. (Excellent pedagogical resource)</li>
      </ul>
      
      <p style="margin-top: 1rem;"><strong>Software Implementations:</strong></p>
      <ul style="padding-left: 1.5rem; margin: 0.5rem 0;">
        <li><strong>Stan:</strong> <a href="https://mc-stan.org/" style="color: #667eea;">mc-stan.org</a> 
        - Uses NUTS (No-U-Turn Sampler), an adaptive HMC variant</li>
        <li><strong>PyMC:</strong> <a href="https://www.pymc.io/" style="color: #667eea;">pymc.io</a> 
        - Python probabilistic programming with HMC/NUTS</li>
        <li><strong>NumPyro:</strong> JAX-based probabilistic programming with efficient HMC</li>
      </ul>
    </div>
  </div>

</div>

  <!-- Shared utilities -->
  <script src="js/shared-utils.js"></script>

  <!-- Algorithm-specific code -->
  <script src="js/hmc.js"></script>

</body>
</html>